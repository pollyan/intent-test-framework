#!/usr/bin/env python3
"""
è½»é‡çº§äº‘ç«¯æ‰§è¡ŒæœåŠ¡
åŸºäºMidSceneJSå®ç°æ„å›¾é©±åŠ¨çš„æµ‹è¯•æ‰§è¡Œï¼Œé’ˆå¯¹å…è´¹äº‘æœåŠ¡å™¨ä¼˜åŒ–
é›†æˆèµ„æºç®¡ç†å’Œæ™ºèƒ½å›é€€æœºåˆ¶
"""

import asyncio
import json
import os
import uuid
import subprocess
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import base64
import requests
from pathlib import Path
import tempfile
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LightweightCloudExecutor:
    """è½»é‡çº§äº‘ç«¯æ‰§è¡Œå™¨ - åŸºäºMidSceneJSçš„æ„å›¾é©±åŠ¨æµ‹è¯•"""
    
    def __init__(self):
        self.midscene_server = None
        self.server_port = 3001
        self.ai_config = self._load_ai_config()
        self.execution_timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
        self.max_memory_mb = 400  # æœ€å¤§å†…å­˜é™åˆ¶ 400MB
        self.optimization_config = None
        
    def _load_ai_config(self) -> Dict[str, str]:
        """åŠ è½½AIé…ç½®"""
        return {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "base_url": os.getenv("OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
            "model": os.getenv("MIDSCENE_MODEL_NAME", "qwen-vl-max-latest"),
            "timeout": "30000"  # 30ç§’è¶…æ—¶
        }
    
    def set_optimization_config(self, config: Dict[str, Any]):
        """è®¾ç½®ä¼˜åŒ–é…ç½®"""
        self.optimization_config = config
    
    async def _start_lightweight_midscene_server(self) -> bool:
        """å¯åŠ¨è½»é‡çº§MidSceneJSæœåŠ¡å™¨"""
        try:
            logger.info("ğŸš€ å¯åŠ¨è½»é‡çº§MidSceneJSæœåŠ¡å™¨...")
            
            # åˆ›å»ºä¸´æ—¶çš„è½»é‡çº§æœåŠ¡å™¨è„šæœ¬
            server_script = await self._create_lightweight_server_script()
            
            # å¯åŠ¨Node.jsæœåŠ¡å™¨
            self.midscene_server = subprocess.Popen(
                ['node', server_script],
                env={
                    **os.environ,
                    'OPENAI_API_KEY': self.ai_config['api_key'],
                    'OPENAI_BASE_URL': self.ai_config['base_url'],
                    'MIDSCENE_MODEL_NAME': self.ai_config['model'],
                    'NODE_OPTIONS': '--max-old-space-size=256'  # é™åˆ¶Node.jså†…å­˜
                },
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            await asyncio.sleep(3)
            
            # éªŒè¯æœåŠ¡å™¨çŠ¶æ€
            if await self._verify_server_health():
                logger.info("âœ… MidSceneJSæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
                return True
            else:
                logger.error("âŒ MidSceneJSæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨MidSceneJSæœåŠ¡å™¨å¼‚å¸¸: {e}")
            return False
    
    async def _create_lightweight_server_script(self) -> str:
        """åˆ›å»ºè½»é‡çº§æœåŠ¡å™¨è„šæœ¬"""
        # è·å–ä¼˜åŒ–é…ç½®
        browser_args = []
        viewport = {"width": 1024, "height": 768}
        
        if self.optimization_config:
            browser_args = self.optimization_config.get("browser_args", [])
            viewport = self.optimization_config.get("viewport", viewport)
        
        # é»˜è®¤ä¼˜åŒ–å‚æ•°
        if not browser_args:
            browser_args = [
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--single-process'
            ]
        
        script_content = f"""
// è½»é‡çº§MidSceneJSæœåŠ¡å™¨ - é’ˆå¯¹äº‘ç«¯èµ„æºä¼˜åŒ–
const express = require('express');
const {{ PlaywrightAgent }} = require('@midscene/web');
const {{ chromium }} = require('playwright');

const app = express();
const port = {self.server_port};

app.use(express.json({{ limit: '10mb' }}));

let browser = null;
let page = null;
let agent = null;

// å¯åŠ¨è½»é‡çº§æµè§ˆå™¨
async function initLightweightBrowser() {{
    if (!browser) {{
        browser = await chromium.launch({{
            headless: true,
            args: {json.dumps(browser_args)}
        }});
    }}
    
    if (!page) {{
        const context = await browser.newContext({{
            viewport: {json.dumps(viewport)},
            deviceScaleFactor: 1
        }});
        page = await context.newPage();
        
        // åˆå§‹åŒ–MidSceneJS AI Agent
        const config = {{
            modelName: process.env.MIDSCENE_MODEL_NAME,
            apiKey: process.env.OPENAI_API_KEY,
            baseUrl: process.env.OPENAI_BASE_URL
        }};
        
        agent = new PlaywrightAgent(page, {{ aiModel: config }});
    }}
    
    return {{ page, agent }};
}}

// å¥åº·æ£€æŸ¥
app.get('/health', (req, res) => {{
    res.json({{ status: 'healthy', timestamp: new Date().toISOString() }});
}});

// æ„å›¾é©±åŠ¨æ‰§è¡Œæ¥å£
app.post('/ai-action', async (req, res) => {{
    try {{
        const {{ action, params }} = req.body;
        await initLightweightBrowser();
        
        let result = {{ success: false, data: null, error: null }};
        
        switch (action) {{
            case 'navigate':
                await page.goto(params.url, {{ waitUntil: 'networkidle' }});
                result = {{ success: true, data: 'Navigation completed' }};
                break;
                
            case 'ai_action':
                const actionResult = await agent.action(params.prompt);
                result = {{ success: true, data: actionResult }};
                break;
                
            case 'ai_query':
                const queryResult = await agent.query(params.prompt);
                result = {{ success: true, data: queryResult }};
                break;
                
            case 'ai_assert':
                const assertResult = await agent.assert(params.prompt);
                result = {{ success: true, data: assertResult }};
                break;
                
            case 'screenshot':
                const screenshot = await page.screenshot({{ 
                    type: 'png',
                    quality: {self.optimization_config.get('screenshot_quality', 80) if self.optimization_config else 80}
                }});
                result = {{ success: true, data: screenshot.toString('base64') }};
                break;
                
            default:
                result = {{ success: false, error: `Unsupported action: ${{action}}` }};
        }}
        
        res.json(result);
    }} catch (error) {{
        console.error('AI Action Error:', error);
        res.status(500).json({{ success: false, error: error.message }});
    }}
}});

// æ‰¹é‡æ‰§è¡Œæ¥å£ - å‡å°‘HTTPè¯·æ±‚æ¬¡æ•°
app.post('/ai-batch', async (req, res) => {{
    try {{
        const {{ actions }} = req.body;
        await initLightweightBrowser();
        
        const results = [];
        
        for (const actionData of actions) {{
            const {{ action, params }} = actionData;
            
            try {{
                let result = {{ success: false, data: null, error: null }};
                
                switch (action) {{
                    case 'navigate':
                        await page.goto(params.url, {{ waitUntil: 'networkidle' }});
                        result = {{ success: true, data: 'Navigation completed' }};
                        break;
                        
                    case 'ai_action':
                        const actionResult = await agent.action(params.prompt);
                        result = {{ success: true, data: actionResult }};
                        break;
                        
                    case 'ai_query':
                        const queryResult = await agent.query(params.prompt);
                        result = {{ success: true, data: queryResult }};
                        break;
                        
                    case 'ai_assert':
                        const assertResult = await agent.assert(params.prompt);
                        result = {{ success: true, data: assertResult }};
                        break;
                        
                    default:
                        result = {{ success: false, error: `Unsupported action: ${{action}}` }};
                }}
                
                results.push(result);
                
            }} catch (error) {{
                results.push({{ success: false, error: error.message }});
            }}
        }}
        
        res.json({{ success: true, results }});
    }} catch (error) {{
        console.error('AI Batch Error:', error);
        res.status(500).json({{ success: false, error: error.message }});
    }}
}});

// æ¸…ç†èµ„æº
app.post('/cleanup', async (req, res) => {{
    try {{
        if (page) await page.close();
        if (browser) await browser.close();
        page = null;
        browser = null;
        agent = null;
        res.json({{ success: true }});
    }} catch (error) {{
        res.status(500).json({{ success: false, error: error.message }});
    }}
}});

// å¯åŠ¨æœåŠ¡å™¨
app.listen(port, () => {{
    console.log(`ğŸš€ è½»é‡çº§MidSceneJSæœåŠ¡å™¨å¯åŠ¨ - ç«¯å£: ${{port}}`);
}});

// è¿›ç¨‹é€€å‡ºæ—¶æ¸…ç†
process.on('SIGINT', async () => {{
    if (browser) await browser.close();
    process.exit(0);
}});
"""
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.js', delete=False)
        temp_file.write(script_content)
        temp_file.close()
        
        return temp_file.name
    
    async def _verify_server_health(self) -> bool:
        """éªŒè¯æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get(f'http://localhost:{self.server_port}/health') as response:
                    return response.status == 200
        except:
            return False
    
    async def _make_ai_request(self, action: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """å‘é€AIè¯·æ±‚åˆ°MidSceneJSæœåŠ¡å™¨"""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f'http://localhost:{self.server_port}/ai-action',
                    json={'action': action, 'params': params},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    return await response.json()
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _make_batch_request(self, actions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‘é€æ‰¹é‡AIè¯·æ±‚åˆ°MidSceneJSæœåŠ¡å™¨"""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f'http://localhost:{self.server_port}/ai-batch',
                    json={'actions': actions},
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    return await response.json()
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def execute_intent_step(self, step: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ„å›¾é©±åŠ¨çš„æµ‹è¯•æ­¥éª¤"""
        action = step.get("action")
        params = step.get("params", {})
        description = step.get("description", action)
        
        result = {
            "success": False,
            "action": action,
            "description": description,
            "screenshot": "",
            "ai_response": "",
            "error": None
        }
        
        try:
            logger.info(f"ğŸ”„ æ‰§è¡Œæ„å›¾æ­¥éª¤: {description}")
            
            # åº”ç”¨æ­¥éª¤å»¶è¿Ÿä¼˜åŒ–
            if self.optimization_config:
                delay = self.optimization_config.get("step_delay", 0.5)
                await asyncio.sleep(delay)
            
            # åŸºäºMidSceneJSçš„æ„å›¾é©±åŠ¨æ‰§è¡Œ
            if action == "navigate":
                ai_result = await self._make_ai_request("navigate", {"url": params.get("url")})
                
            elif action == "ai_input":
                # æ„å›¾é©±åŠ¨è¾“å…¥ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‰¾åˆ°è¾“å…¥æ¡†å¹¶è¾“å…¥
                locate_prompt = params.get("locate", "è¾“å…¥æ¡†")
                text = params.get("text", "")
                ai_result = await self._make_ai_request("ai_action", {
                    "prompt": f"åœ¨{locate_prompt}ä¸­è¾“å…¥'{text}'"
                })
                
            elif action == "ai_tap":
                # æ„å›¾é©±åŠ¨ç‚¹å‡»ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‰¾åˆ°å¹¶ç‚¹å‡»å…ƒç´ 
                element_desc = params.get("element", "æŒ‰é’®")
                ai_result = await self._make_ai_request("ai_action", {
                    "prompt": f"ç‚¹å‡»{element_desc}"
                })
                
            elif action == "ai_assert":
                # æ„å›¾é©±åŠ¨æ–­è¨€ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€éªŒè¯é¡µé¢çŠ¶æ€
                assertion = params.get("assertion", "")
                ai_result = await self._make_ai_request("ai_assert", {
                    "prompt": assertion
                })
                
            elif action == "ai_query":
                # æ„å›¾é©±åŠ¨æŸ¥è¯¢ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢é¡µé¢ä¿¡æ¯
                query = params.get("query", "")
                ai_result = await self._make_ai_request("ai_query", {
                    "prompt": query
                })
                
            elif action == "ai_wait_for":
                # æ„å›¾é©±åŠ¨ç­‰å¾…ï¼šç­‰å¾…ç‰¹å®šæ¡ä»¶æ»¡è¶³
                condition = params.get("condition", "")
                ai_result = await self._make_ai_request("ai_query", {
                    "prompt": f"æ£€æŸ¥æ˜¯å¦{condition}"
                })
                
            else:
                ai_result = {"success": False, "error": f"ä¸æ”¯æŒçš„æ„å›¾æ“ä½œ: {action}"}
            
            # å¤„ç†ç»“æœ
            if ai_result.get("success"):
                result["success"] = True
                result["ai_response"] = str(ai_result.get("data", ""))
            else:
                result["error"] = ai_result.get("error", "æœªçŸ¥é”™è¯¯")
            
            # è·å–æˆªå›¾ - æ ¹æ®ä¼˜åŒ–é…ç½®å†³å®šæ˜¯å¦æˆªå›¾
            should_screenshot = True
            if self.optimization_config:
                # åœ¨é«˜å†…å­˜å‹åŠ›ä¸‹å‡å°‘æˆªå›¾
                should_screenshot = self.optimization_config.get("screenshot_quality", 80) > 0
            
            if should_screenshot:
                screenshot_result = await self._make_ai_request("screenshot", {})
                if screenshot_result.get("success"):
                    result["screenshot"] = screenshot_result.get("data", "")
            
        except Exception as e:
            result["error"] = str(e)
            logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
        
        return result
    
    async def execute_batch_steps(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ‰§è¡Œæµ‹è¯•æ­¥éª¤ - å‡å°‘HTTPè¯·æ±‚æ¬¡æ•°"""
        try:
            # æ„å»ºæ‰¹é‡è¯·æ±‚
            batch_actions = []
            for step in steps:
                action = step.get("action")
                params = step.get("params", {})
                
                if action == "navigate":
                    batch_actions.append({
                        "action": "navigate",
                        "params": {"url": params.get("url")}
                    })
                elif action == "ai_input":
                    locate_prompt = params.get("locate", "è¾“å…¥æ¡†")
                    text = params.get("text", "")
                    batch_actions.append({
                        "action": "ai_action",
                        "params": {"prompt": f"åœ¨{locate_prompt}ä¸­è¾“å…¥'{text}'"}
                    })
                elif action == "ai_tap":
                    element_desc = params.get("element", "æŒ‰é’®")
                    batch_actions.append({
                        "action": "ai_action",
                        "params": {"prompt": f"ç‚¹å‡»{element_desc}"}
                    })
                elif action == "ai_assert":
                    assertion = params.get("assertion", "")
                    batch_actions.append({
                        "action": "ai_assert",
                        "params": {"prompt": assertion}
                    })
                elif action == "ai_query":
                    query = params.get("query", "")
                    batch_actions.append({
                        "action": "ai_query",
                        "params": {"prompt": query}
                    })
            
            # å‘é€æ‰¹é‡è¯·æ±‚
            batch_result = await self._make_batch_request(batch_actions)
            
            # å¤„ç†æ‰¹é‡ç»“æœ
            results = []
            if batch_result.get("success"):
                batch_results = batch_result.get("results", [])
                for i, (step, ai_result) in enumerate(zip(steps, batch_results)):
                    result = {
                        "success": ai_result.get("success", False),
                        "action": step.get("action"),
                        "description": step.get("description", step.get("action")),
                        "screenshot": "",
                        "ai_response": str(ai_result.get("data", "")),
                        "error": ai_result.get("error")
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°å•æ­¥æ‰§è¡Œ
            return [await self.execute_intent_step(step) for step in steps]
    
    async def execute_testcase(self, testcase_data: Dict[str, Any], mode: str = "headless") -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æ„å›¾é©±åŠ¨æµ‹è¯•ç”¨ä¾‹"""
        execution_id = str(uuid.uuid4())
        
        execution_result = {
            "execution_id": execution_id,
            "testcase_name": testcase_data.get("name", "æœªçŸ¥æµ‹è¯•ç”¨ä¾‹"),
            "mode": mode,
            "status": "running",
            "start_time": datetime.utcnow().isoformat(),
            "steps": [],
            "screenshots": [],
            "success_count": 0,
            "total_count": 0
        }
        
        try:
            # å¯åŠ¨è½»é‡çº§MidSceneJSæœåŠ¡å™¨
            if not await self._start_lightweight_midscene_server():
                execution_result["status"] = "failed"
                execution_result["error"] = "MidSceneJSæœåŠ¡å™¨å¯åŠ¨å¤±è´¥"
                return execution_result
            
            # è§£ææµ‹è¯•æ­¥éª¤
            steps = json.loads(testcase_data.get("steps", "[]"))
            execution_result["total_count"] = len(steps)
            
            # å†³å®šæ˜¯å¦ä½¿ç”¨æ‰¹é‡æ‰§è¡Œ
            use_batch = len(steps) > 3 and self.optimization_config and self.optimization_config.get("use_batch", True)
            
            if use_batch:
                # æ‰¹é‡æ‰§è¡Œ
                logger.info(f"ğŸ“¦ æ‰¹é‡æ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤")
                step_results = await self.execute_batch_steps(steps)
                execution_result["steps"] = step_results
                
                # è®¡ç®—æˆåŠŸæ•°
                for step_result in step_results:
                    if step_result["success"]:
                        execution_result["success_count"] += 1
                
            else:
                # é€æ­¥æ‰§è¡Œæ„å›¾é©±åŠ¨çš„æµ‹è¯•æ­¥éª¤
                for i, step in enumerate(steps):
                    logger.info(f"ğŸ“ æ‰§è¡Œæ­¥éª¤ {i+1}/{len(steps)}: {step.get('description', step.get('action'))}")
                    
                    step_result = await self.execute_intent_step(step)
                    execution_result["steps"].append(step_result)
                    
                    if step_result["success"]:
                        execution_result["success_count"] += 1
                        logger.info(f"âœ… æ­¥éª¤ {i+1} æˆåŠŸ")
                    else:
                        logger.warning(f"âŒ æ­¥éª¤ {i+1} å¤±è´¥: {step_result.get('error')}")
                    
                    # é€‚å½“çš„æ­¥éª¤é—´å»¶è¿Ÿ
                    delay = 0.5
                    if self.optimization_config:
                        delay = self.optimization_config.get("step_delay", 0.5)
                    await asyncio.sleep(delay)
            
            # æ·»åŠ æˆªå›¾åˆ°å†å²
            for i, step_result in enumerate(execution_result["steps"]):
                if step_result["screenshot"]:
                    execution_result["screenshots"].append({
                        "step": i + 1,
                        "description": step_result["description"],
                        "screenshot": step_result["screenshot"]
                    })
            
            # è®¡ç®—æ‰§è¡Œç»“æœ
            execution_result["status"] = "completed"
            execution_result["end_time"] = datetime.utcnow().isoformat()
            
            success_rate = (execution_result["success_count"] / execution_result["total_count"]) * 100
            execution_result["success_rate"] = success_rate
            
            logger.info(f"ğŸ‰ æ„å›¾é©±åŠ¨æµ‹è¯•æ‰§è¡Œå®Œæˆï¼æˆåŠŸç‡: {success_rate:.1f}%")
            
        except Exception as e:
            execution_result["status"] = "failed"
            execution_result["error"] = str(e)
            execution_result["end_time"] = datetime.utcnow().isoformat()
            logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup()
        
        return execution_result
    
    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # é€šçŸ¥æœåŠ¡å™¨æ¸…ç†
            await self._make_ai_request("cleanup", {})
            
            # ç»ˆæ­¢æœåŠ¡å™¨è¿›ç¨‹
            if self.midscene_server:
                self.midscene_server.terminate()
                try:
                    await asyncio.wait_for(self.midscene_server.wait(), timeout=5)
                except asyncio.TimeoutError:
                    self.midscene_server.kill()
                    await self.midscene_server.wait()
            
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºé”™: {e}")

# å‘åå…¼å®¹çš„ç±»å
CloudExecutionService = LightweightCloudExecutor

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """æµ‹è¯•äº‘ç«¯æ‰§è¡ŒæœåŠ¡"""
    service = CloudExecutionService()
    
    # ç¤ºä¾‹ä¼˜åŒ–é…ç½®
    optimization_config = {
        "browser_args": [
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-web-security",
            "--disable-features=VizDisplayCompositor",
            "--single-process"
        ],
        "viewport": {"width": 1024, "height": 768},
        "screenshot_quality": 80,
        "step_delay": 0.5,
        "use_batch": True
    }
    service.set_optimization_config(optimization_config)
    
    # ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹
    testcase = {
        "name": "ç™¾åº¦æœç´¢æµ‹è¯•",
        "steps": json.dumps([
            {
                "action": "navigate",
                "params": {"url": "https://www.baidu.com"},
                "description": "è®¿é—®ç™¾åº¦é¦–é¡µ"
            },
            {
                "action": "ai_input",
                "params": {"text": "AIæµ‹è¯•", "locate": "æœç´¢æ¡†"},
                "description": "è¾“å…¥æœç´¢å…³é”®è¯"
            },
            {
                "action": "ai_tap",
                "params": {"prompt": "æœç´¢æŒ‰é’®"},
                "description": "ç‚¹å‡»æœç´¢"
            },
            {
                "action": "ai_assert",
                "params": {"prompt": "é¡µé¢æ˜¾ç¤ºäº†æœç´¢ç»“æœ"},
                "description": "éªŒè¯æœç´¢ç»“æœ"
            }
        ])
    }
    
    # æ‰§è¡Œæµ‹è¯•
    result = await service.execute_testcase(testcase, mode="headless")
    print(f"\næ‰§è¡Œç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    asyncio.run(main())
