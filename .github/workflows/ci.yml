name: CI/CD Pipeline

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main ]

env:
  PYTHON_VERSION: 3.11
  NODE_VERSION: 18

jobs:
  test:
    name: API Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: intent_test_framework_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r web_gui/requirements.txt
        
        # 安装测试相关依赖
        pip install pytest pytest-cov pytest-mock pytest-asyncio pytest-html
        pip install coverage[toml] pytest-xdist

    - name: Install Node.js dependencies
      run: npm ci

    - name: Install Playwright browsers
      run: npx playwright install chromium --with-deps

    - name: Setup test environment variables
      run: |
        cat << EOF > .env.test
        # Test Database Configuration
        DATABASE_URL=postgresql://test_user:test_password@localhost:5432/intent_test_framework_test
        
        # Flask Configuration
        FLASK_ENV=testing
        DEBUG=false
        SECRET_KEY=test_secret_key_for_ci
        
        # AI Service Configuration (using mock in tests)
        OPENAI_API_KEY=test_key
        OPENAI_BASE_URL=http://mock-ai-service
        MIDSCENE_MODEL_NAME=mock-model
        
        # Test Configuration
        TESTING=true
        WTF_CSRF_ENABLED=false
        LOGIN_DISABLED=true
        
        # Logging Configuration
        LOG_LEVEL=WARNING
        EOF

    - name: Initialize test database
      run: |
        export $(cat .env.test | xargs)
        python -c "
        from web_gui.app_enhanced import create_app
        from web_gui.models import db
        
        app = create_app()
        with app.app_context():
            db.create_all()
            print('✅ Test database initialized')
        "

    - name: Run API tests with coverage
      run: |
        export $(cat .env.test | xargs)
        python -m pytest tests/api/ \
          --verbose \
          --tb=short \
          --cov=web_gui \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --maxfail=10 \
          --durations=10 \
          --junit-xml=test-results.xml

    - name: Run Node.js proxy tests
      run: |
        npm test -- --coverage --watchAll=false

    - name: Upload coverage to Codecov
      if: success()
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: api-tests
        name: API Tests Coverage

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          test-results.xml
          htmlcov/
          coverage/

    - name: Comment test coverage on PR
      if: github.event_name == 'pull_request'
      uses: py-cov-action/python-coverage-comment-action@v3
      with:
        GITHUB_TOKEN: ${{ github.token }}

  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run quality checks
      run: |
        python scripts/quality_check.py --check-only

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security scanning tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit semgrep

    - name: Run security checks
      run: |
        # Check for known vulnerabilities in dependencies
        safety check --json
        
        # Run static security analysis
        bandit -r web_gui/ -f json -o bandit-report.json || true
        
        # Upload security reports
        echo "Security scan completed"

    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json

  performance-test:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark

    - name: Run performance tests
      run: |
        python -m pytest tests/api/ \
          -m "not slow" \
          --benchmark-only \
          --benchmark-json=benchmark.json

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: benchmark.json