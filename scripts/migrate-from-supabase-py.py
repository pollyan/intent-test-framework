#!/usr/bin/env python3
"""
Supabase PostgreSQLåˆ°æœ¬åœ°PostgreSQLæ•°æ®è¿ç§»è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python scripts/migrate-from-supabase-py.py
"""

import os
import sys
from sqlalchemy import create_engine, text, MetaData
from datetime import datetime

# Supabaseè¿æ¥ä¿¡æ¯
SUPABASE_URL = "postgresql://postgres.jzmqsuxphksbulrbhebp:Shunlian04@aws-0-ap-northeast-1.pooler.supabase.com:6543/postgres"

# æœ¬åœ°PostgreSQLè¿æ¥ä¿¡æ¯ (Dockerå®¹å™¨ï¼Œhost.docker.internalæˆ–localhost:5432)
# Dockeré»˜è®¤æ— å¯†ç ï¼Œä½¿ç”¨trustè®¤è¯
LOCAL_URL = "postgresql://intent_user:change_me_in_production@localhost:5432/intent_test"

def log(message, level="INFO"):
    """è¾“å‡ºæ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

def migrate():
    """æ‰§è¡Œæ•°æ®è¿ç§»"""
    log("========================================")
    log("æ•°æ®åº“è¿ç§»ï¼šSupabase â†’ æœ¬åœ° PostgreSQL")
    log("========================================")
    
    try:
        # 1. è¿æ¥æ•°æ®åº“
        log("è¿æ¥ Supabase...")
        source_engine = create_engine(SUPABASE_URL, connect_args={"connect_timeout": 15})
        
        log("è¿æ¥æœ¬åœ° PostgreSQL...")
        target_engine = create_engine(LOCAL_URL)
        
        # æµ‹è¯•è¿æ¥
        with source_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        log("âœ… Supabase è¿æ¥æˆåŠŸ")
        
        with target_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        log("âœ… æœ¬åœ° PostgreSQL è¿æ¥æˆåŠŸ")
        
        # 2. æ ¸å¿ƒä¸šåŠ¡è¡¨åˆ—è¡¨ - æ‰©å±•åŒ…å«æ‰€æœ‰æœ‰æ•°æ®çš„è¡¨
        core_tables = [
            'test_cases',           # 14æ¡
            'execution_history',    # 214æ¡
            'step_executions',      # 2æ¡
            'templates',            # 1æ¡
            'execution_variables',  # 1æ¡
            'requirements_ai_configs',  # 3æ¡
            'requirements_sessions',    # 168æ¡
            'requirements_messages'     # 478æ¡
        ]
        log(f"å°†è¿ç§» {len(core_tables)} ä¸ªæ ¸å¿ƒè¡¨: {', '.join(core_tables)}")
        
        # 3. å¤‡ä»½æœ¬åœ°æ•°æ®
        log("å¤‡ä»½æœ¬åœ°æ•°æ®...")
        backup_dir = "./database_backups"
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"{backup_dir}/local_backup_before_migration_{timestamp}.sql"
        os.system(f"docker exec intent-test-db pg_dump -U intent_user intent_test > {backup_file}")
        log(f"âœ… æœ¬åœ°æ•°æ®å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # 4. æ¸…ç©ºæœ¬åœ°æ ¸å¿ƒè¡¨
        log("æ¸…ç©ºæœ¬åœ°è¡¨...")
        with target_engine.connect() as conn:
            # ç®€å•æ¸…ç©ºè¡¨ï¼Œä¸ä½¿ç”¨ session_replication_roleï¼ˆéœ€è¦è¶…çº§ç”¨æˆ·æƒé™ï¼‰
            for table_name in reversed(core_tables):
                try:
                    conn.execute(text(f"TRUNCATE TABLE {table_name} CASCADE"))
                    log(f"  æ¸…ç©ºè¡¨: {table_name}")
                except Exception as e:
                    log(f"  è·³è¿‡è¡¨ {table_name}: {e}", "WARNING")
            
            conn.commit()
        
        log("âœ… æœ¬åœ°è¡¨å·²å‡†å¤‡å¥½")
        
        # 5. å¤åˆ¶æ•°æ®
        log("å¼€å§‹å¤åˆ¶æ•°æ®...")
        total_rows = 0
        
        for table_name in core_tables:
            # ä» Supabase è¯»å– (æŒ‡å®š public schema)
            with source_engine.connect() as source_conn:
                result = source_conn.execute(text(f'SELECT * FROM public."{table_name}"'))
                rows = result.fetchall()
                columns = result.keys()
            
            if not rows:
                log(f"  è¡¨ {table_name}: æ— æ•°æ®")
                continue
            
            # å†™å…¥æœ¬åœ°
            column_list = ", ".join(columns)
            placeholders = ", ".join([f":{col}" for col in columns])
            insert_sql = f"INSERT INTO {table_name} ({column_list}) VALUES ({placeholders})"
            
            with target_engine.connect() as target_conn:
                for row in rows:
                    row_dict = dict(zip(columns, row))
                    target_conn.execute(text(insert_sql), row_dict)
                target_conn.commit()
            
            log(f"  âœ… è¡¨ {table_name}: {len(rows)} æ¡è®°å½•")
            total_rows += len(rows)
        
        log(f"âœ… æ•°æ®å¤åˆ¶å®Œæˆï¼Œå…± {total_rows} æ¡è®°å½•")
        
        # 6. éªŒè¯
        log("éªŒè¯æ•°æ®...")
        for table_name in core_tables:
            with source_engine.connect() as source_conn:
                source_count = source_conn.execute(
                    text(f'SELECT COUNT(*) FROM public."{table_name}"')
                ).scalar()
            
            with target_engine.connect() as target_conn:
                target_count = target_conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name}")
                ).scalar()
            
            if source_count == target_count:
                log(f"  âœ… {table_name}: {source_count} æ¡è®°å½•")
            else:
                log(f"  âš ï¸  {table_name}: æº={source_count}, æœ¬åœ°={target_count}", "WARNING")
        
        log("========================================")
        log("ğŸ‰ è¿ç§»å®Œæˆï¼")
        log("========================================")
        log(f"å¤‡ä»½æ–‡ä»¶: {backup_file}")
        log("========================================")
        
        return True
        
    except Exception as e:
        log(f"âŒ è¿ç§»å¤±è´¥: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = migrate()
    sys.exit(0 if success else 1)
