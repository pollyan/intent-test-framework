#!/usr/bin/env python3
"""
Intent Test Framework - æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

ç”¨äºç›‘æ§å’Œåˆ†ææµ‹è¯•å¥—ä»¶çš„æ€§èƒ½è¡¨ç°ï¼Œè¯†åˆ«æ…¢é€Ÿæµ‹è¯•å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
"""

import json
import time
import sys
import subprocess
import argparse
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime


class TestBenchmark:
    """æµ‹è¯•åŸºå‡†åˆ†æå™¨"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.results_file = self.project_root / "benchmark_results.json"
        self.threshold_slow = 5.0  # æ…¢é€Ÿæµ‹è¯•é˜ˆå€¼ï¼ˆç§’ï¼‰
        self.threshold_fast = 1.0  # å¿«é€Ÿæµ‹è¯•é˜ˆå€¼ï¼ˆç§’ï¼‰

    def run_benchmark(
        self, test_path: str = "tests/api", parallel: bool = False
    ) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•: {test_path}")
        start_time = time.time()

        # æ„å»ºpytestå‘½ä»¤
        cmd = [
            "python",
            "-m",
            "pytest",
            test_path,
            "--durations=0",  # æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•çš„æ‰§è¡Œæ—¶é—´
            "--tb=no",  # ä¸æ˜¾ç¤ºtracebackä»¥å‡å°‘è¾“å‡º
            "-v",
            (
                "--benchmark-json=benchmark.json"
                if self._has_benchmark_plugin()
                else "--json-report"
            ),
        ]

        if parallel and self._has_xdist():
            cmd.extend(["-n", "auto", "--dist", "worksteal"])

        # è¿è¡Œæµ‹è¯•
        try:
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            )

            execution_time = time.time() - start_time

            # è§£æç»“æœ
            benchmark_data = self._parse_pytest_output(result.stdout, result.stderr)
            benchmark_data.update(
                {
                    "total_execution_time": execution_time,
                    "parallel_enabled": parallel,
                    "test_path": test_path,
                    "timestamp": datetime.now().isoformat(),
                    "exit_code": result.returncode,
                }
            )

            return benchmark_data

        except subprocess.TimeoutExpired:
            print("âŒ æµ‹è¯•æ‰§è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
            return {"error": "timeout", "execution_time": 300}
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": str(e)}

    def _parse_pytest_output(self, stdout: str, stderr: str) -> Dict[str, Any]:
        """è§£æpytestè¾“å‡ºï¼Œæå–æ€§èƒ½æ•°æ®"""
        data = {
            "tests": [],
            "slow_tests": [],
            "fast_tests": [],
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "errors": 0,
            "warnings": [],
        }

        # è§£ææµ‹è¯•ç»“æœç»Ÿè®¡
        for line in stdout.split("\n"):
            if "passed" in line and ("failed" in line or "error" in line):
                # è§£ææµ‹è¯•ç»Ÿè®¡è¡Œï¼Œå¦‚ï¼š25 passed, 2 failed in 10.20s
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == "passed":
                        data["passed"] = int(parts[i - 1])
                    elif part == "failed":
                        data["failed"] = int(parts[i - 1])
                    elif part == "error":
                        data["errors"] = int(parts[i - 1])

            # è§£ææ…¢é€Ÿæµ‹è¯•
            if "SLOWEST TEST DURATIONS" in line or "slowest durations" in line:
                slow_tests = self._extract_slow_tests(stdout[stdout.find(line) :])
                data["slow_tests"].extend(slow_tests)

        data["total_tests"] = data["passed"] + data["failed"] + data["errors"]

        # åˆ†ç±»æµ‹è¯•
        for test in data["slow_tests"]:
            if test["duration"] > self.threshold_slow:
                test["category"] = "slow"
            elif test["duration"] < self.threshold_fast:
                test["category"] = "fast"
            else:
                test["category"] = "medium"

        return data

    def _extract_slow_tests(self, durations_section: str) -> List[Dict[str, Any]]:
        """ä»pytestè¾“å‡ºä¸­æå–æ…¢é€Ÿæµ‹è¯•ä¿¡æ¯"""
        tests = []
        lines = durations_section.split("\n")

        for line in lines:
            # åŒ¹é…æ ¼å¼: 1.23s call tests/api/test_example.py::test_function
            if "s call " in line and "::" in line:
                try:
                    parts = line.split("s call ")
                    if len(parts) == 2:
                        duration = float(parts[0].strip())
                        test_path = parts[1].strip()

                        tests.append(
                            {"test": test_path, "duration": duration, "phase": "call"}
                        )
                except ValueError:
                    continue

        return tests

    def analyze_performance(self, benchmark_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š"""
        analysis = {
            "summary": {},
            "recommendations": [],
            "slow_test_analysis": {},
            "performance_score": 0,
        }

        # åŸºæœ¬ç»Ÿè®¡
        total_time = benchmark_data.get("total_execution_time", 0)
        total_tests = benchmark_data.get("total_tests", 0)
        slow_tests = benchmark_data.get("slow_tests", [])

        analysis["summary"] = {
            "total_execution_time": total_time,
            "total_tests": total_tests,
            "average_test_time": total_time / max(total_tests, 1),
            "slow_test_count": len(
                [t for t in slow_tests if t["duration"] > self.threshold_slow]
            ),
            "fast_test_count": len(
                [t for t in slow_tests if t["duration"] < self.threshold_fast]
            ),
        }

        # æ€§èƒ½è¯„åˆ†ï¼ˆ0-100ï¼‰
        avg_time = analysis["summary"]["average_test_time"]
        slow_ratio = analysis["summary"]["slow_test_count"] / max(total_tests, 1)

        # åŸºç¡€åˆ†æ•°ï¼šåŸºäºå¹³å‡æµ‹è¯•æ—¶é—´
        if avg_time < 0.5:
            time_score = 100
        elif avg_time < 1.0:
            time_score = 80
        elif avg_time < 2.0:
            time_score = 60
        else:
            time_score = max(0, 60 - (avg_time - 2) * 10)

        # æ…¢é€Ÿæµ‹è¯•æƒ©ç½š
        slow_penalty = slow_ratio * 30

        analysis["performance_score"] = max(0, int(time_score - slow_penalty))

        # ç”Ÿæˆå»ºè®®
        analysis["recommendations"] = self._generate_recommendations(
            analysis["summary"]
        )

        return analysis

    def _generate_recommendations(self, summary: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        avg_time = summary["average_test_time"]
        slow_count = summary["slow_test_count"]
        fast_count = summary["fast_test_count"]
        total_tests = summary["total_tests"]

        # å¹³å‡æ—¶é—´å»ºè®®
        if avg_time > 2.0:
            recommendations.append("âš ï¸ å¹³å‡æµ‹è¯•æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•æ•°æ®å‡†å¤‡å’Œæ¸…ç†é€»è¾‘")
        elif avg_time > 1.0:
            recommendations.append("ğŸ’¡ è€ƒè™‘ä½¿ç”¨æµ‹è¯•æ•°æ®å·¥å‚å’Œå†…å­˜æ•°æ®åº“åŠ é€Ÿæµ‹è¯•")

        # æ…¢é€Ÿæµ‹è¯•å»ºè®®
        if slow_count > total_tests * 0.1:  # è¶…è¿‡10%çš„æµ‹è¯•æ˜¯æ…¢é€Ÿçš„
            recommendations.append(
                "ğŸŒ å‘ç°è¿‡å¤šæ…¢é€Ÿæµ‹è¯•ï¼Œå»ºè®®ä½¿ç”¨@pytest.mark.slowæ ‡è®°å¹¶åœ¨å¿«é€Ÿæµ‹è¯•ä¸­æ’é™¤"
            )
            recommendations.append("ğŸ“Š è€ƒè™‘å°†æ…¢é€Ÿæµ‹è¯•æ‹†åˆ†ä¸ºæ›´å°çš„æµ‹è¯•å•å…ƒ")

        # å¹¶è¡ŒåŒ–å»ºè®®
        if total_tests > 10 and not recommendations:
            recommendations.append(
                "ğŸš€ æµ‹è¯•æ•°é‡è¾ƒå¤šï¼Œå»ºè®®å¯ç”¨å¹¶è¡Œæµ‹è¯•: pip install pytest-xdist"
            )

        # å¿«é€Ÿæµ‹è¯•æ¯”ä¾‹å»ºè®®
        fast_ratio = fast_count / max(total_tests, 1)
        if fast_ratio < 0.5:
            recommendations.append("âš¡ å¿«é€Ÿæµ‹è¯•æ¯”ä¾‹è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•è®¾è®¡å’Œæ•°æ®å‡†å¤‡")

        if not recommendations:
            recommendations.append("âœ… æµ‹è¯•æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")

        return recommendations

    def save_results(self, benchmark_data: Dict[str, Any], analysis: Dict[str, Any]):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        results = {
            "benchmark": benchmark_data,
            "analysis": analysis,
            "generated_at": datetime.now().isoformat(),
        }

        # è¯»å–å†å²æ•°æ®
        history = []
        if self.results_file.exists():
            try:
                with open(self.results_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        history = data
                    else:
                        history = [data]  # å…¼å®¹æ—§æ ¼å¼
            except:
                pass

        # æ·»åŠ æ–°ç»“æœï¼ˆä¿ç•™æœ€è¿‘10æ¬¡ï¼‰
        history.append(results)
        history = history[-10:]

        # ä¿å­˜
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def print_report(self, analysis: Dict[str, Any]):
        """æ‰“å°æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)

        summary = analysis["summary"]
        print(f"ğŸ“ˆ æ€§èƒ½è¯„åˆ†: {analysis['performance_score']}/100")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {summary['total_execution_time']:.2f}ç§’")
        print(f"ğŸ§ª æµ‹è¯•æ€»æ•°: {summary['total_tests']}")
        print(f"ğŸ“Š å¹³å‡æµ‹è¯•æ—¶é—´: {summary['average_test_time']:.3f}ç§’")
        print(f"ğŸŒ æ…¢é€Ÿæµ‹è¯•: {summary['slow_test_count']} (>{self.threshold_slow}s)")
        print(f"âš¡ å¿«é€Ÿæµ‹è¯•: {summary['fast_test_count']} (<{self.threshold_fast}s)")

        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(analysis["recommendations"], 1):
            print(f"  {i}. {rec}")

        print("\n" + "=" * 60)

    def _has_benchmark_plugin(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®‰è£…äº†pytest-benchmarkæ’ä»¶"""
        try:
            import pytest_benchmark

            return True
        except ImportError:
            return False

    def _has_xdist(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®‰è£…äº†pytest-xdistæ’ä»¶"""
        try:
            import xdist

            return True
        except ImportError:
            return False


def main():
    parser = argparse.ArgumentParser(
        description="Intent Test Framework æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"
    )
    parser.add_argument(
        "--path", default="tests/api", help="æµ‹è¯•è·¯å¾„ (é»˜è®¤: tests/api)"
    )
    parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œæµ‹è¯•")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜ç»“æœåˆ°æ–‡ä»¶")
    parser.add_argument("--report-only", action="store_true", help="åªæ˜¾ç¤ºæœ€æ–°æŠ¥å‘Š")

    args = parser.parse_args()

    benchmark = TestBenchmark()

    if args.report_only:
        # åªæ˜¾ç¤ºæœ€æ–°æŠ¥å‘Š
        if benchmark.results_file.exists():
            with open(benchmark.results_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list) and data:
                    latest = data[-1]
                    benchmark.print_report(latest["analysis"])
                else:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°å†å²åŸºå‡†æµ‹è¯•æ•°æ®")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŸºå‡†æµ‹è¯•ç»“æœæ–‡ä»¶")
        return

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_data = benchmark.run_benchmark(args.path, args.parallel)

    if "error" in benchmark_data:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {benchmark_data['error']}")
        sys.exit(1)

    # åˆ†ææ€§èƒ½
    analysis = benchmark.analyze_performance(benchmark_data)

    # æ˜¾ç¤ºæŠ¥å‘Š
    benchmark.print_report(analysis)

    # ä¿å­˜ç»“æœ
    if args.save:
        benchmark.save_results(benchmark_data, analysis)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {benchmark.results_file}")


if __name__ == "__main__":
    main()
